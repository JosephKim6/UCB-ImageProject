{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEC_Lab_IntroLevel\n",
      "GEC_Lab_Level2\n",
      "Kaggle-planet-test-tif.torrent\n",
      "Kaggle-planet-train-tif.torrent\n",
      "Report 1 for ML Program on image Liang Zhang.docx\n",
      "Report 2 for ML Program on image Liang Zhang.docx\n",
      "UCB-ImageProject\n",
      "sample_submission_v2.csv\n",
      "sample_submission_v2.csv.zip\n",
      "test-jpg-additional.tar\n",
      "test-jpg-additional.tar.7z\n",
      "test-jpg.tar\n",
      "test-jpg.tar.7z\n",
      "test-tif-v2.tar.7z\n",
      "test_v2_file_mapping.csv.zip\n",
      "train-jpg.tar\n",
      "train-jpg.tar.7z\n",
      "train-tif-v2.tar.7z\n",
      "train_v2.csv.zip\n",
      "~$port 1 for ML Program on image Liang Zhang.docx\n",
      "~$port 2 for ML Program on image Liang Zhang.docx\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40479/40479 [00:10<00:00, 3844.92it/s]\n",
      "100%|██████████| 61191/61191 [00:22<00:00, 2674.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../\"]).decode(\"utf8\"))\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from keras import optimizers\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import fbeta_score\n",
    "import time\n",
    "\n",
    "x_train = []\n",
    "x_test = []\n",
    "y_train = []\n",
    "\n",
    "df_train = pd.read_csv('train_v2.csv')\n",
    "df_test = pd.read_csv('sample_submission_v2.csv')\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "labels = list(set(flatten([l.split(' ') for l in df_train['tags'].values])))\n",
    "\n",
    "labels = ['blow_down',\n",
    " 'bare_ground',\n",
    " 'conventional_mine',\n",
    " 'blooming',\n",
    " 'cultivation',\n",
    " 'artisinal_mine',\n",
    " 'haze',\n",
    " 'primary',\n",
    " 'slash_burn',\n",
    " 'habitation',\n",
    " 'clear',\n",
    " 'road',\n",
    " 'selective_logging',\n",
    " 'partly_cloudy',\n",
    " 'agriculture',\n",
    " 'water',\n",
    " 'cloudy']\n",
    "\n",
    "label_map = {'agriculture': 14,\n",
    " 'artisinal_mine': 5,\n",
    " 'bare_ground': 1,\n",
    " 'blooming': 3,\n",
    " 'blow_down': 0,\n",
    " 'clear': 10,\n",
    " 'cloudy': 16,\n",
    " 'conventional_mine': 2,\n",
    " 'cultivation': 4,\n",
    " 'habitation': 9,\n",
    " 'haze': 6,\n",
    " 'partly_cloudy': 13,\n",
    " 'primary': 7,\n",
    " 'road': 11,\n",
    " 'selective_logging': 12,\n",
    " 'slash_burn': 8,\n",
    " 'water': 15}\n",
    "\n",
    "count = 0\n",
    "for f, tags in tqdm(df_train.values, miniters=1000):\n",
    "    if count < 2000:\n",
    "        img = cv2.imread('./train-jpg/{}.jpg'.format(f))\n",
    "        targets = np.zeros(17)\n",
    "        for t in tags.split(' '):\n",
    "            targets[label_map[t]] = 1 \n",
    "        x_train.append(cv2.resize(img, (64, 64)))\n",
    "        y_train.append(targets)\n",
    "        count += 1\n",
    "\n",
    "count = 0\n",
    "for f, tags in tqdm(df_test.values, miniters=1000):\n",
    "    if count < 5000:\n",
    "        img = cv2.imread('./test-jpg/{}.jpg'.format(f))\n",
    "        x_test.append(cv2.resize(img, (64, 64)))\n",
    "        count += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 64, 64, 3)\n",
      "(2000, 17)\n",
      "Start KFold number 1 from 4\n",
      "Split train:  1500 1500\n",
      "Split valid:  500 500\n",
      "Train on 1500 samples, validate on 500 samples\n",
      "Epoch 1/20\n",
      "73s - loss: 0.8354 - acc: 0.5099 - val_loss: 5.3216 - val_acc: 0.3094\n",
      "Epoch 2/20\n",
      "71s - loss: 0.8749 - acc: 0.5277 - val_loss: 5.3790 - val_acc: 0.2739\n",
      "Epoch 3/20\n",
      "72s - loss: 0.7780 - acc: 0.5664 - val_loss: 4.4221 - val_acc: 0.1555\n",
      "Epoch 4/20\n",
      "497s - loss: 0.7158 - acc: 0.6084 - val_loss: 2.5140 - val_acc: 0.1186\n",
      "Epoch 5/20\n",
      "75s - loss: 0.6225 - acc: 0.6758 - val_loss: 1.5628 - val_acc: 0.2028\n",
      "Epoch 6/20\n",
      "73s - loss: 0.5290 - acc: 0.7522 - val_loss: 1.3264 - val_acc: 0.2118\n",
      "Epoch 7/20\n",
      "72s - loss: 0.4519 - acc: 0.8140 - val_loss: 0.9337 - val_acc: 0.3209\n",
      "Epoch 8/20\n",
      "72s - loss: 0.3957 - acc: 0.8519 - val_loss: 0.5584 - val_acc: 0.7607\n",
      "Epoch 9/20\n",
      "73s - loss: 0.3453 - acc: 0.8799 - val_loss: 0.6258 - val_acc: 0.6000\n",
      "Epoch 10/20\n",
      "75s - loss: 0.3239 - acc: 0.8897 - val_loss: 0.6259 - val_acc: 0.6946\n",
      "Epoch 11/20\n",
      "72s - loss: 0.3108 - acc: 0.8921 - val_loss: 0.5266 - val_acc: 0.7447\n",
      "Epoch 12/20\n",
      "80s - loss: 0.3008 - acc: 0.8962 - val_loss: 0.3922 - val_acc: 0.9061\n",
      "Epoch 13/20\n",
      "81s - loss: 0.2984 - acc: 0.8980 - val_loss: 0.3199 - val_acc: 0.9061\n",
      "Epoch 14/20\n",
      "103s - loss: 0.2894 - acc: 0.8998 - val_loss: 0.2886 - val_acc: 0.9061\n",
      "Epoch 15/20\n",
      "99s - loss: 0.2900 - acc: 0.8992 - val_loss: 0.2907 - val_acc: 0.9061\n",
      "Epoch 16/20\n",
      "95s - loss: 0.2844 - acc: 0.8989 - val_loss: 0.2667 - val_acc: 0.9061\n",
      "Epoch 17/20\n",
      "93s - loss: 0.2834 - acc: 0.9004 - val_loss: 0.2644 - val_acc: 0.9061\n",
      "Epoch 18/20\n",
      "96s - loss: 0.2822 - acc: 0.9011 - val_loss: 0.2623 - val_acc: 0.9061\n",
      "Epoch 19/20\n",
      "113s - loss: 0.2767 - acc: 0.9022 - val_loss: 0.2603 - val_acc: 0.9061\n",
      "Epoch 20/20\n",
      "114s - loss: 0.2789 - acc: 0.9014 - val_loss: 0.2639 - val_acc: 0.9061\n",
      "Train on 1500 samples, validate on 500 samples\n",
      "Epoch 1/5\n",
      "2532s - loss: 0.2784 - acc: 0.9016 - val_loss: 0.2606 - val_acc: 0.9061\n",
      "Epoch 2/5\n",
      "1115s - loss: 0.2775 - acc: 0.9024 - val_loss: 0.2575 - val_acc: 0.9061\n",
      "Epoch 3/5\n",
      "134s - loss: 0.2763 - acc: 0.9016 - val_loss: 0.2555 - val_acc: 0.9061\n",
      "Epoch 4/5\n",
      "102s - loss: 0.2780 - acc: 0.9024 - val_loss: 0.2543 - val_acc: 0.9061\n",
      "Epoch 5/5\n",
      "106s - loss: 0.2739 - acc: 0.9030 - val_loss: 0.2541 - val_acc: 0.9061\n",
      "Train on 1500 samples, validate on 500 samples\n",
      "Epoch 1/5\n",
      "96s - loss: 0.2738 - acc: 0.9029 - val_loss: 0.2539 - val_acc: 0.9061\n",
      "Epoch 2/5\n",
      "92s - loss: 0.2743 - acc: 0.9030 - val_loss: 0.2539 - val_acc: 0.9061\n",
      "Epoch 3/5\n",
      "96s - loss: 0.2761 - acc: 0.9031 - val_loss: 0.2539 - val_acc: 0.9061\n",
      "Epoch 4/5\n",
      "98s - loss: 0.2743 - acc: 0.9025 - val_loss: 0.2539 - val_acc: 0.9061\n",
      "Epoch 5/5\n",
      "94s - loss: 0.2763 - acc: 0.9023 - val_loss: 0.2540 - val_acc: 0.9061\n",
      "final fbeta_score:  0.663555931849\n",
      "Start KFold number 2 from 4\n",
      "Split train:  1500 1500\n",
      "Split valid:  500 500\n",
      "Train on 1500 samples, validate on 500 samples\n",
      "Epoch 1/20\n",
      "108s - loss: 0.6884 - acc: 0.5896 - val_loss: 0.6497 - val_acc: 0.9025\n",
      "Epoch 2/20\n",
      "107s - loss: 0.6193 - acc: 0.8958 - val_loss: 0.5741 - val_acc: 0.9025\n",
      "Epoch 3/20\n",
      "100s - loss: 0.5256 - acc: 0.9069 - val_loss: 0.4634 - val_acc: 0.9025\n",
      "Epoch 4/20\n",
      "105s - loss: 0.4097 - acc: 0.9071 - val_loss: 0.3581 - val_acc: 0.9025\n",
      "Epoch 5/20\n",
      "98s - loss: 0.3228 - acc: 0.9071 - val_loss: 0.2999 - val_acc: 0.9025\n",
      "Epoch 6/20\n",
      "1922s - loss: 0.2820 - acc: 0.9071 - val_loss: 0.2794 - val_acc: 0.9025\n",
      "Epoch 7/20\n",
      "107s - loss: 0.2619 - acc: 0.9071 - val_loss: 0.2669 - val_acc: 0.9025\n",
      "Epoch 9/20\n",
      "113s - loss: 0.2587 - acc: 0.9071 - val_loss: 0.2648 - val_acc: 0.9025\n",
      "Epoch 10/20\n",
      "103s - loss: 0.2578 - acc: 0.9071 - val_loss: 0.2637 - val_acc: 0.9025\n",
      "Epoch 11/20\n",
      "117s - loss: 0.2562 - acc: 0.9071 - val_loss: 0.2629 - val_acc: 0.9025\n",
      "Epoch 12/20\n",
      "103s - loss: 0.2559 - acc: 0.9071 - val_loss: 0.2625 - val_acc: 0.9025\n",
      "Epoch 13/20\n",
      "123s - loss: 0.2555 - acc: 0.9071 - val_loss: 0.2622 - val_acc: 0.9025\n",
      "Epoch 14/20\n",
      "104s - loss: 0.2549 - acc: 0.9071 - val_loss: 0.2620 - val_acc: 0.9025\n",
      "Epoch 15/20\n",
      "100s - loss: 0.2547 - acc: 0.9071 - val_loss: 0.2619 - val_acc: 0.9025\n",
      "Epoch 16/20\n",
      "104s - loss: 0.2550 - acc: 0.9071 - val_loss: 0.2618 - val_acc: 0.9025\n",
      "Epoch 17/20\n",
      "110s - loss: 0.2545 - acc: 0.9071 - val_loss: 0.2617 - val_acc: 0.9025\n",
      "Epoch 18/20\n",
      "110s - loss: 0.2541 - acc: 0.9071 - val_loss: 0.2617 - val_acc: 0.9025\n",
      "Epoch 19/20\n",
      "112s - loss: 0.2542 - acc: 0.9071 - val_loss: 0.2616 - val_acc: 0.9025\n",
      "Epoch 20/20\n",
      "93s - loss: 0.2544 - acc: 0.9071 - val_loss: 0.2617 - val_acc: 0.9025\n",
      "Train on 1500 samples, validate on 500 samples\n",
      "Epoch 1/5\n",
      "102s - loss: 0.2540 - acc: 0.9071 - val_loss: 0.2616 - val_acc: 0.9025\n",
      "Epoch 2/5\n",
      "105s - loss: 0.2533 - acc: 0.9071 - val_loss: 0.2616 - val_acc: 0.9025\n",
      "Epoch 3/5\n",
      "98s - loss: 0.2539 - acc: 0.9071 - val_loss: 0.2616 - val_acc: 0.9025\n",
      "Epoch 4/5\n",
      "110s - loss: 0.2539 - acc: 0.9071 - val_loss: 0.2616 - val_acc: 0.9025\n",
      "Train on 1500 samples, validate on 500 samples\n",
      "Epoch 1/5\n",
      "102s - loss: 0.2541 - acc: 0.9071 - val_loss: 0.2616 - val_acc: 0.9025\n",
      "Epoch 2/5\n",
      "104s - loss: 0.2542 - acc: 0.9071 - val_loss: 0.2616 - val_acc: 0.9025\n",
      "Epoch 3/5\n",
      "96s - loss: 0.2536 - acc: 0.9071 - val_loss: 0.2616 - val_acc: 0.9025\n",
      "Epoch 4/5\n",
      "96s - loss: 0.2540 - acc: 0.9071 - val_loss: 0.2616 - val_acc: 0.9025\n",
      "Epoch 5/5\n",
      "97s - loss: 0.2537 - acc: 0.9071 - val_loss: 0.2616 - val_acc: 0.9025\n",
      "final fbeta_score:  0.664398358229\n",
      "Start KFold number 3 from 4\n",
      "Split train:  1500 1500\n",
      "Split valid:  500 500\n",
      "Train on 1500 samples, validate on 500 samples\n",
      "Epoch 1/20\n",
      "114s - loss: 0.7009 - acc: 0.5472 - val_loss: 0.6533 - val_acc: 0.9066\n",
      "Epoch 2/20\n",
      "101s - loss: 0.6276 - acc: 0.8874 - val_loss: 0.5876 - val_acc: 0.9066\n",
      "Epoch 3/20\n",
      "107s - loss: 0.5440 - acc: 0.9057 - val_loss: 0.4834 - val_acc: 0.9066\n",
      "Epoch 4/20\n",
      "96s - loss: 0.4341 - acc: 0.9057 - val_loss: 0.3729 - val_acc: 0.9066\n",
      "Epoch 5/20\n",
      "101s - loss: 0.3391 - acc: 0.9057 - val_loss: 0.3029 - val_acc: 0.9066\n",
      "Epoch 6/20\n",
      "103s - loss: 0.2901 - acc: 0.9057 - val_loss: 0.2753 - val_acc: 0.9066\n",
      "Epoch 7/20\n",
      "104s - loss: 0.2714 - acc: 0.9057 - val_loss: 0.2654 - val_acc: 0.9066\n",
      "Epoch 8/20\n",
      "106s - loss: 0.2650 - acc: 0.9057 - val_loss: 0.2611 - val_acc: 0.9066\n",
      "Epoch 9/20\n",
      "124s - loss: 0.2618 - acc: 0.9057 - val_loss: 0.2589 - val_acc: 0.9066\n",
      "Epoch 10/20\n",
      "111s - loss: 0.2603 - acc: 0.9057 - val_loss: 0.2575 - val_acc: 0.9066\n",
      "Epoch 11/20\n",
      "97s - loss: 0.2595 - acc: 0.9057 - val_loss: 0.2565 - val_acc: 0.9066\n",
      "Epoch 12/20\n",
      "100s - loss: 0.2583 - acc: 0.9057 - val_loss: 0.2559 - val_acc: 0.9066\n",
      "Epoch 13/20\n",
      "96s - loss: 0.2579 - acc: 0.9057 - val_loss: 0.2555 - val_acc: 0.9066\n",
      "Epoch 14/20\n",
      "97s - loss: 0.2575 - acc: 0.9057 - val_loss: 0.2551 - val_acc: 0.9066\n",
      "Epoch 15/20\n",
      "102s - loss: 0.2582 - acc: 0.9057 - val_loss: 0.2549 - val_acc: 0.9066\n",
      "Epoch 16/20\n",
      "97s - loss: 0.2570 - acc: 0.9057 - val_loss: 0.2547 - val_acc: 0.9066\n",
      "Epoch 17/20\n",
      "96s - loss: 0.2569 - acc: 0.9057 - val_loss: 0.2545 - val_acc: 0.9066\n",
      "Epoch 18/20\n",
      "98s - loss: 0.2567 - acc: 0.9057 - val_loss: 0.2545 - val_acc: 0.9066\n",
      "Epoch 19/20\n",
      "96s - loss: 0.2566 - acc: 0.9057 - val_loss: 0.2544 - val_acc: 0.9066\n",
      "Epoch 20/20\n",
      "97s - loss: 0.2565 - acc: 0.9057 - val_loss: 0.2543 - val_acc: 0.9066\n",
      "Train on 1500 samples, validate on 500 samples\n",
      "Epoch 1/5\n",
      "113s - loss: 0.2564 - acc: 0.9057 - val_loss: 0.2543 - val_acc: 0.9066\n",
      "Epoch 2/5\n",
      "101s - loss: 0.2565 - acc: 0.9057 - val_loss: 0.2543 - val_acc: 0.9066\n",
      "Epoch 3/5\n",
      "103s - loss: 0.2568 - acc: 0.9057 - val_loss: 0.2542 - val_acc: 0.9066\n",
      "Epoch 4/5\n",
      "104s - loss: 0.2563 - acc: 0.9057 - val_loss: 0.2542 - val_acc: 0.9066\n",
      "Epoch 5/5\n",
      "111s - loss: 0.2565 - acc: 0.9057 - val_loss: 0.2542 - val_acc: 0.9066\n",
      "Train on 1500 samples, validate on 500 samples\n",
      "Epoch 1/5\n",
      "127s - loss: 0.2568 - acc: 0.9057 - val_loss: 0.2542 - val_acc: 0.9066\n",
      "Epoch 2/5\n",
      "110s - loss: 0.2567 - acc: 0.9057 - val_loss: 0.2542 - val_acc: 0.9066\n",
      "Epoch 3/5\n",
      "109s - loss: 0.2564 - acc: 0.9057 - val_loss: 0.2542 - val_acc: 0.9066\n",
      "Epoch 4/5\n",
      "103s - loss: 0.2564 - acc: 0.9057 - val_loss: 0.2542 - val_acc: 0.9066\n",
      "Epoch 5/5\n",
      "94s - loss: 0.2564 - acc: 0.9057 - val_loss: 0.2542 - val_acc: 0.9066\n",
      "final fbeta_score:  0.669816468254\n",
      "Start KFold number 4 from 4\n",
      "Split train:  1500 1500\n",
      "Split valid:  500 500\n",
      "Train on 1500 samples, validate on 500 samples\n",
      "Epoch 1/20\n",
      "109s - loss: 0.7118 - acc: 0.5470 - val_loss: 0.6564 - val_acc: 0.9085\n",
      "Epoch 2/20\n",
      "89s - loss: 0.6328 - acc: 0.8629 - val_loss: 0.5966 - val_acc: 0.9085\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91s - loss: 0.5557 - acc: 0.9051 - val_loss: 0.4992 - val_acc: 0.9085\n",
      "Epoch 4/20\n",
      "94s - loss: 0.4503 - acc: 0.9051 - val_loss: 0.3878 - val_acc: 0.9085\n",
      "Epoch 5/20\n",
      "96s - loss: 0.3509 - acc: 0.9051 - val_loss: 0.3100 - val_acc: 0.9085\n",
      "Epoch 6/20\n",
      "115s - loss: 0.2958 - acc: 0.9051 - val_loss: 0.2763 - val_acc: 0.9085\n",
      "Epoch 7/20\n",
      "103s - loss: 0.2742 - acc: 0.9051 - val_loss: 0.2650 - val_acc: 0.9085\n",
      "Epoch 8/20\n",
      "103s - loss: 0.2663 - acc: 0.9051 - val_loss: 0.2608 - val_acc: 0.9085\n",
      "Epoch 9/20\n",
      "104s - loss: 0.2624 - acc: 0.9051 - val_loss: 0.2590 - val_acc: 0.9085\n",
      "Epoch 10/20\n",
      "93s - loss: 0.2604 - acc: 0.9051 - val_loss: 0.2579 - val_acc: 0.9085\n",
      "Epoch 11/20\n",
      "94s - loss: 0.2593 - acc: 0.9051 - val_loss: 0.2571 - val_acc: 0.9085\n",
      "Epoch 12/20\n",
      "119s - loss: 0.2588 - acc: 0.9051 - val_loss: 0.2565 - val_acc: 0.9085\n",
      "Epoch 13/20\n",
      "122s - loss: 0.2580 - acc: 0.9051 - val_loss: 0.2560 - val_acc: 0.9085\n",
      "Epoch 14/20\n",
      "107s - loss: 0.2574 - acc: 0.9051 - val_loss: 0.2557 - val_acc: 0.9085\n",
      "Epoch 15/20\n",
      "104s - loss: 0.2575 - acc: 0.9051 - val_loss: 0.2555 - val_acc: 0.9085\n",
      "Epoch 16/20\n",
      "99s - loss: 0.2568 - acc: 0.9051 - val_loss: 0.2553 - val_acc: 0.9085\n",
      "Epoch 17/20\n",
      "98s - loss: 0.2568 - acc: 0.9051 - val_loss: 0.2553 - val_acc: 0.9085\n",
      "Epoch 18/20\n",
      "99s - loss: 0.2565 - acc: 0.9051 - val_loss: 0.2552 - val_acc: 0.9085\n",
      "Epoch 19/20\n",
      "93s - loss: 0.2568 - acc: 0.9051 - val_loss: 0.2551 - val_acc: 0.9085\n",
      "Epoch 20/20\n",
      "101s - loss: 0.2568 - acc: 0.9051 - val_loss: 0.2551 - val_acc: 0.9085\n",
      "Train on 1500 samples, validate on 500 samples\n",
      "Epoch 1/5\n",
      "117s - loss: 0.2567 - acc: 0.9051 - val_loss: 0.2550 - val_acc: 0.9085\n",
      "Epoch 2/5\n",
      "100s - loss: 0.2563 - acc: 0.9051 - val_loss: 0.2550 - val_acc: 0.9085\n",
      "Epoch 3/5\n",
      "100s - loss: 0.2566 - acc: 0.9051 - val_loss: 0.2550 - val_acc: 0.9085\n",
      "Epoch 4/5\n",
      "102s - loss: 0.2562 - acc: 0.9051 - val_loss: 0.2550 - val_acc: 0.9085\n",
      "Epoch 5/5\n",
      "101s - loss: 0.2563 - acc: 0.9051 - val_loss: 0.2550 - val_acc: 0.9085\n",
      "Train on 1500 samples, validate on 500 samples\n",
      "Epoch 1/5\n",
      "107s - loss: 0.2561 - acc: 0.9051 - val_loss: 0.2550 - val_acc: 0.9085\n",
      "Epoch 2/5\n",
      "101s - loss: 0.2562 - acc: 0.9051 - val_loss: 0.2550 - val_acc: 0.9085\n",
      "Epoch 3/5\n",
      "101s - loss: 0.2565 - acc: 0.9051 - val_loss: 0.2550 - val_acc: 0.9085\n",
      "Epoch 4/5\n",
      "111s - loss: 0.2565 - acc: 0.9051 - val_loss: 0.2549 - val_acc: 0.9085\n",
      "Epoch 5/5\n",
      "95s - loss: 0.2561 - acc: 0.9051 - val_loss: 0.2549 - val_acc: 0.9085\n",
      "final fbeta_score:  0.667003968254\n"
     ]
    }
   ],
   "source": [
    "y_train = np.array(y_train, np.uint8)\n",
    "x_train = np.array(x_train, np.float32)/255.\n",
    "x_test  = np.array(x_test, np.float32)/255.\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "#nfolds = 5\n",
    "nfolds = 4\n",
    "\n",
    "num_fold = 0\n",
    "sum_score = 0\n",
    "\n",
    "yfull_test = []\n",
    "yfull_train =[]\n",
    "\n",
    "kf = KFold(len(y_train), n_folds=nfolds, shuffle=True, random_state=1)\n",
    "\n",
    "for train_index, test_index in kf:\n",
    "        start_time_model_fitting = time.time()\n",
    "        \n",
    "        X_train = x_train[train_index]\n",
    "        Y_train = y_train[train_index]\n",
    "        X_valid = x_train[test_index]\n",
    "        Y_valid = y_train[test_index]\n",
    "\n",
    "        num_fold += 1\n",
    "        print('Start KFold number {} from {}'.format(num_fold, nfolds))\n",
    "        print('Split train: ', len(X_train), len(Y_train))\n",
    "        print('Split valid: ', len(X_valid), len(Y_valid))\n",
    "        \n",
    "        kfold_weights_path = os.path.join('', 'weights_kfold_' + str(num_fold) + '.h5')\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(BatchNormalization(input_shape=(64, 64,3)))\n",
    "        model.add(Conv2D(32, kernel_size=(3, 3),padding='same', activation='relu'))\n",
    "        model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Conv2D(64, kernel_size=(3, 3),padding='same', activation='relu'))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        model.add(Conv2D(128, kernel_size=(3, 3),padding='same', activation='relu'))\n",
    "        model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        model.add(Conv2D(256, kernel_size=(3, 3),padding='same', activation='relu'))\n",
    "        model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(17, activation='sigmoid'))\n",
    "\n",
    "        epochs_arr = [20, 5, 5]\n",
    "        learn_rates = [0.001, 0.0001, 0.00001]\n",
    "\n",
    "        for learn_rate, epochs in zip(learn_rates, epochs_arr):\n",
    "            opt  = optimizers.Adam(lr=learn_rate)\n",
    "            model.compile(loss='binary_crossentropy', # We NEED binary here, since categorical_crossentropy l1 norms the output before calculating loss.\n",
    "                          optimizer=opt,\n",
    "                          metrics=['accuracy'])\n",
    "            callbacks = [EarlyStopping(monitor='val_loss', patience=2, verbose=0),\n",
    "            ModelCheckpoint(kfold_weights_path, monitor='val_loss', save_best_only=True, verbose=0)]\n",
    "\n",
    "            model.fit(x = X_train, y= Y_train, validation_data=(X_valid, Y_valid),\n",
    "                  batch_size=128,verbose=2, epochs=epochs,callbacks=callbacks,shuffle=True)\n",
    "        \n",
    "        if os.path.isfile(kfold_weights_path):\n",
    "            model.load_weights(kfold_weights_path)\n",
    "        \n",
    "        p_valid = model.predict(X_valid, batch_size = 128, verbose=2)\n",
    "        print(\"final fbeta_score: \", fbeta_score(Y_valid, np.array(p_valid) > 0.2, beta=2, average='samples'))\n",
    "\n",
    "        p_train = model.predict(x_train, batch_size =128, verbose=2)\n",
    "        yfull_train.append(p_train)\n",
    "        \n",
    "        p_test = model.predict(x_test, batch_size = 128, verbose=2)\n",
    "        yfull_test.append(p_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
